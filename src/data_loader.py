"""
Module de chargement des donn√©es pour la maintenance pr√©dictive.
Charge le dataset NASA C-MAPSS et pr√©pare les donn√©es pour l'analyse.
"""

import pandas as pd
import numpy as np
from typing import Tuple, Dict, Optional
import requests
from io import StringIO
import os
import yaml

class DataLoader:
    """
    Classe pour charger et pr√©parer les donn√©es de maintenance pr√©dictive.
    
    Attributes:
        config (dict): Configuration du projet
        data (dict): Donn√©es charg√©es
    """
    
    def __init__(self, config_path: str = "config.yaml"):
        """
        Initialise le chargeur de donn√©es.
        
        Args:
            config_path: Chemin vers le fichier de configuration
        """
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)
        
        self.data = {}
        print("‚úÖ Chargeur de donn√©es initialis√©")
    
    def load_nasa_data(self, use_cache: bool = True) -> Dict[str, pd.DataFrame]:
        """
        Charge le dataset NASA C-MAPSS depuis GitHub ou cache local.
        
        Args:
            use_cache: Si True, utilise les donn√©es en cache si disponibles
            
        Returns:
            Dict avec les DataFrames train, test et truth
        """
        print("üì• Chargement des donn√©es NASA C-MAPSS...")
        
        # Chemins des fichiers
        cache_dir = self.config['paths']['data']
        raw_dir = os.path.join(cache_dir, 'raw')
        os.makedirs(raw_dir, exist_ok=True)
        
        train_path = os.path.join(raw_dir, 'train.csv')
        test_path = os.path.join(raw_dir, 'test.csv')
        truth_path = os.path.join(raw_dir, 'truth.csv')
        
        # V√©rifier le cache
        if use_cache and all(os.path.exists(p) for p in [train_path, test_path, truth_path]):
            print("  Utilisation des donn√©es en cache...")
            train_df = pd.read_csv(train_path)
            test_df = pd.read_csv(test_path)
            truth_df = pd.read_csv(truth_path)
        else:
            print("  T√©l√©chargement depuis GitHub...")
            # URLs des donn√©es
            urls = {
                'train': 'https://raw.githubusercontent.com/ashishpatel26/Predictive-Maintenance-using-LSTM/master/PM_train.txt',
                'test': 'https://raw.githubusercontent.com/ashishpatel26/Predictive-Maintenance-using-LSTM/master/PM_test.txt',
                'truth': 'https://raw.githubusercontent.com/ashishpatel26/Predictive-Maintenance-using-LSTM/master/PM_truth.txt'
            }
            
            # T√©l√©charger les donn√©es
            train_df = self._download_data(urls['train'])
            test_df = self._download_data(urls['test'])
            truth_df = self._download_data(urls['truth'])
            
            # Sauvegarder en cache
            train_df.to_csv(train_path, index=False)
            test_df.to_csv(test_path, index=False)
            truth_df.to_csv(truth_path, index=False)
            print("  Donn√©es sauvegard√©es en cache")
        
        # Nettoyer et nommer les colonnes
        train_df = self._clean_data(train_df)
        test_df = self._clean_data(test_df)
        
        columns = ['unit_id', 'time_cycle']
        columns += [f'operational_setting_{i}' for i in range(1, 4)]
        columns += [f'sensor_measurement_{i}' for i in range(1, 22)]
        
        train_df.columns = columns[:train_df.shape[1]]
        test_df.columns = columns[:test_df.shape[1]]
        
        # Pr√©parer les donn√©es de test avec RUL
        truth_df.columns = ['RUL']
        test_df = pd.concat([test_df, truth_df], axis=1)
        
        self.data = {
            'train': train_df,
            'test': test_df,
            'truth': truth_df
        }
        
        print(f"‚úÖ Donn√©es charg√©es: {len(train_df)} lignes d'entra√Ænement")
        print(f"                     {len(test_df)} lignes de test")
        
        return self.data
    
    def _download_data(self, url: str) -> pd.DataFrame:
        """
        T√©l√©charge des donn√©es depuis une URL.
        
        Args:
            url: URL des donn√©es
            
        Returns:
            DataFrame avec les donn√©es
        """
        response = requests.get(url)
        df = pd.read_csv(StringIO(response.text), sep=" ", header=None)
        df.dropna(axis=1, how='all', inplace=True)
        return df
    
    def _clean_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Nettoie les donn√©es brutes.
        
        Args:
            df: DataFrame √† nettoyer
            
        Returns:
            DataFrame nettoy√©
        """
        # Supprimer les colonnes vides
        df = df.dropna(axis=1, how='all')
        
        # Supprimer les doublons
        df = df.drop_duplicates()
        
        return df
    
    def calculate_rul(self, df: pd.DataFrame, label: str = 'train') -> pd.DataFrame:
        """
        Calcule le RUL (Remaining Useful Life) pour chaque observation.
        
        Args:
            df: DataFrame avec les donn√©es
            label: 'train' ou 'test'
            
        Returns:
            DataFrame avec colonne RUL ajout√©e
        """
        print(f"üìä Calcul du RUL pour les donn√©es {label}...")
        
        df_rul = df.copy()
        
        if 'RUL' in df_rul.columns:
            print("  RUL d√©j√† pr√©sent dans les donn√©es")
            return df_rul
        
        # Calculer le cycle maximum pour chaque unit√©
        max_cycle = df_rul.groupby('unit_id')['time_cycle'].max().reset_index()
        max_cycle.columns = ['unit_id', 'max_cycle']
        
        # Fusionner et calculer RUL
        df_rul = df_rul.merge(max_cycle, on='unit_id', how='left')
        df_rul['RUL'] = df_rul['max_cycle'] - df_rul['time_cycle']
        df_rul.drop('max_cycle', axis=1, inplace=True)
        
        print(f"  RUL calcul√©: {df_rul['RUL'].min():.0f} √† {df_rul['RUL'].max():.0f} cycles")
        
        return df_rul
    
    def prepare_sequences(self, df: pd.DataFrame, sequence_length: int = 50) -> Tuple[np.ndarray, np.ndarray]:
        """
        Pr√©pare les s√©quences pour les mod√®les LSTM.
        
        Args:
            df: DataFrame avec les donn√©es
            sequence_length: Longueur des s√©quences
            
        Returns:
            Tuple (X, y) pour l'entra√Ænement
        """
        print(f"üîÑ Pr√©paration des s√©quences (longueur: {sequence_length})...")
        
        sequences = []
        targets = []
        
        # Identifier les colonnes de capteurs
        sensor_cols = [col for col in df.columns if 'sensor' in col]
        
        for unit_id in df['unit_id'].unique():
            unit_data = df[df['unit_id'] == unit_id]
            
            # Trier par cycle temporel
            unit_data = unit_data.sort_values('time_cycle')
            
            # Cr√©er des s√©quences glissantes
            for i in range(len(unit_data) - sequence_length):
                seq = unit_data.iloc[i:i + sequence_length][sensor_cols].values
                target = unit_data.iloc[i + sequence_length]['RUL']
                
                sequences.append(seq)
                targets.append(target)
        
        X = np.array(sequences)
        y = np.array(targets)
        
        print(f"  ‚úÖ {len(sequences)} s√©quences cr√©√©es")
        print(f"  X shape: {X.shape}, y shape: {y.shape}")
        
        return X, y
    
    def get_data_summary(self) -> Dict:
        """
        Retourne un r√©sum√© des donn√©es charg√©es.
        
        Returns:
            Dict avec les statistiques des donn√©es
        """
        if not self.data:
            return {"error": "Aucune donn√©e charg√©e"}
        
        train_df = self.data['train']
        test_df = self.data['test']
        
        summary = {
            'train': {
                'shape': train_df.shape,
                'units': train_df['unit_id'].nunique(),
                'cycles_max': train_df.groupby('unit_id')['time_cycle'].max().mean(),
                'sensors': len([col for col in train_df.columns if 'sensor' in col])
            },
            'test': {
                'shape': test_df.shape,
                'units': test_df['unit_id'].nunique(),
                'has_rul': 'RUL' in test_df.columns
            },
            'features': {
                'sensor_cols': [col for col in train_df.columns if 'sensor' in col][:5],
                'op_setting_cols': [col for col in train_df.columns if 'operational' in col]
            }
        }
        
        return summary

def main():
    """Fonction principale pour tester le module."""
    print("=" * 50)
    print("TEST DU MODULE DE CHARGEMENT DE DONN√âES")
    print("=" * 50)
    
    # Initialiser le chargeur
    loader = DataLoader()
    
    # Charger les donn√©es
    data = loader.load_nasa_data(use_cache=True)
    
    # Calculer RUL pour les donn√©es d'entra√Ænement
    train_with_rul = loader.calculate_rul(data['train'], 'train')
    
    # Pr√©parer les s√©quences
    X, y = loader.prepare_sequences(train_with_rul, sequence_length=50)
    
    # Afficher le r√©sum√©
    summary = loader.get_data_summary()
    print("\nüìã R√âSUM√â DES DONN√âES:")
    print(f"  Donn√©es d'entra√Ænement: {summary['train']['shape']}")
    print(f"  Unit√©s d'entra√Ænement: {summary['train']['units']}")
    print(f"  Capteurs: {summary['train']['sensors']}")
    print(f"  Forme des s√©quences: {X.shape}")
    
    return data

if __name__ == "__main__":
    data = main()
