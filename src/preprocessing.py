"""
Module de pr√©traitement des donn√©es pour la maintenance pr√©dictive.
Nettoyage, normalisation et feature engineering.
"""

import numpy as np
import pandas as pd
from typing import Tuple, Dict, List
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.impute import SimpleImputer
import warnings
warnings.filterwarnings('ignore')

class DataPreprocessor:
    """
    Classe pour le pr√©traitement des donn√©es de maintenance.
    
    Attributes:
        scalers (dict): Dictionnaire des scalers entra√Æn√©s
        imputers (dict): Dictionnaire des imputers
    """
    
    def __init__(self):
        """Initialise le pr√©processeur."""
        self.scalers = {}
        self.imputers = {}
        print("‚úÖ Pr√©processeur initialis√©")
    
    def clean_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Nettoie les donn√©es brutes.
        
        Args:
            df: DataFrame √† nettoyer
            
        Returns:
            DataFrame nettoy√©
        """
        print("üßπ Nettoyage des donn√©es...")
        
        df_clean = df.copy()
        
        # 1. Supprimer les colonnes avec trop de valeurs manquantes
        missing_threshold = 0.5
        cols_to_drop = []
        
        for col in df_clean.columns:
            missing_ratio = df_clean[col].isnull().sum() / len(df_clean)
            if missing_ratio > missing_threshold:
                cols_to_drop.append(col)
        
        if cols_to_drop:
            print(f"  Suppression de {len(cols_to_drop)} colonnes avec >{missing_threshold*100}% de valeurs manquantes")
            df_clean = df_clean.drop(columns=cols_to_drop)
        
        # 2. Imputer les valeurs manquantes
        for col in df_clean.select_dtypes(include=[np.number]).columns:
            if df_clean[col].isnull().any():
                imputer = SimpleImputer(strategy='median')
                df_clean[col] = imputer.fit_transform(df_clean[[col]]).ravel()
                self.imputers[col] = imputer
        
        # 3. Supprimer les doublons
        initial_len = len(df_clean)
        df_clean = df_clean.drop_duplicates()
        duplicates_removed = initial_len - len(df_clean)
        
        if duplicates_removed > 0:
            print(f"  {duplicates_removed} doublons supprim√©s")
        
        print(f"  ‚úÖ Donn√©es nettoy√©es: {df_clean.shape}")
        
        return df_clean
    
    def normalize_features(self, df: pd.DataFrame, method: str = 'standard') -> pd.DataFrame:
        """
        Normalise les caract√©ristiques num√©riques.
        
        Args:
            df: DataFrame √† normaliser
            method: 'standard' (StandardScaler) ou 'minmax' (MinMaxScaler)
            
        Returns:
            DataFrame normalis√©
        """
        print(f"üìè Normalisation des caract√©ristiques (m√©thode: {method})...")
        
        df_norm = df.copy()
        
        # Identifier les colonnes num√©riques √† normaliser
        numeric_cols = df_norm.select_dtypes(include=[np.number]).columns.tolist()
        
        # Exclure certaines colonnes
        exclude_cols = ['unit_id', 'time_cycle', 'RUL']
        numeric_cols = [col for col in numeric_cols if col not in exclude_cols]
        
        if not numeric_cols:
            print("  ‚ö†Ô∏è  Aucune colonne √† normaliser")
            return df_norm
        
        # Appliquer la normalisation
        if method == 'standard':
            scaler = StandardScaler()
        elif method == 'minmax':
            scaler = MinMaxScaler()
        else:
            raise ValueError(f"M√©thode de normalisation inconnue: {method}")
        
        # Normaliser les donn√©es
        df_norm[numeric_cols] = scaler.fit_transform(df_norm[numeric_cols])
        
        # Sauvegarder le scaler
        self.scalers[method] = scaler
        
        print(f"  ‚úÖ {len(numeric_cols)} colonnes normalis√©es")
        
        return df_norm
    
    def create_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Cr√©e de nouvelles caract√©ristiques √† partir des donn√©es existantes.
        
        Args:
            df: DataFrame d'entr√©e
            
        Returns:
            DataFrame avec nouvelles caract√©ristiques
        """
        print("üîß Cr√©ation de caract√©ristiques...")
        
        df_features = df.copy()
        
        # V√©rifier que les colonnes n√©cessaires existent
        if 'time_cycle' not in df_features.columns:
            print("  ‚ö†Ô∏è  Colonne 'time_cycle' non trouv√©e")
            return df_features
        
        # Caract√©ristiques temporelles
        if 'time_cycle' in df_features.columns:
            # Cycle normalis√© par unit√©
            df_features['cycle_norm'] = df_features.groupby('unit_id')['time_cycle'].transform(
                lambda x: (x - x.min()) / (x.max() - x.min()) if x.max() != x.min() else 0
            )
            
            # Diff√©rence avec le cycle pr√©c√©dent
            df_features['cycle_diff'] = df_features.groupby('unit_id')['time_cycle'].diff().fillna(0)
        
        # Caract√©ristiques statistiques par unit√©
        sensor_cols = [col for col in df_features.columns if 'sensor' in col]
        
        if sensor_cols:
            # Moyennes glissantes
            for sensor in sensor_cols[:5]:  # Limiter aux 5 premiers capteurs
                df_features[f'{sensor}_rolling_mean'] = df_features.groupby('unit_id')[sensor].transform(
                    lambda x: x.rolling(window=5, min_periods=1).mean()
                )
                
                df_features[f'{sensor}_rolling_std'] = df_features.groupby('unit_id')[sensor].transform(
                    lambda x: x.rolling(window=5, min_periods=1).std()
                )
        
        # Caract√©ristiques d'ing√©nierie
        if len(sensor_cols) >= 2:
            # Ratio entre capteurs
            df_features['sensor_ratio_1_2'] = df_features[sensor_cols[0]] / (df_features[sensor_cols[1]] + 1e-10)
        
        print(f"  ‚úÖ Caract√©ristiques cr√©√©es: {len(df_features.columns)} colonnes totales")
        
        return df_features
    
    def detect_outliers(self, df: pd.DataFrame, threshold: float = 3.0) -> Dict:
        """
        D√©tecte les outliers dans les donn√©es.
        
        Args:
            df: DataFrame √† analyser
            threshold: Seuil en √©carts-types
            
        Returns:
            Dict avec informations sur les outliers
        """
        print(f"üîç D√©tection des outliers (seuil: {threshold}œÉ)...")
        
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        
        outliers_info = {
            'total_outliers': 0,
            'outliers_per_column': {},
            'percentage_outliers': 0
        }
        
        for col in numeric_cols:
            if col in ['unit_id', 'time_cycle']:
                continue
            
            values = df[col]
            mean = values.mean()
            std = values.std()
            
            if std == 0:
                continue
            
            # Calculer les z-scores
            z_scores = np.abs((values - mean) / std)
            
            # Compter les outliers
            outliers = (z_scores > threshold).sum()
            
            if outliers > 0:
                outliers_info['outliers_per_column'][col] = {
                    'count': int(outliers),
                    'percentage': outliers / len(df) * 100,
                    'mean': float(mean),
                    'std': float(std)
                }
                outliers_info['total_outliers'] += outliers
        
        total_values = len(df) * len(numeric_cols)
        if total_values > 0:
            outliers_info['percentage_outliers'] = outliers_info['total_outliers'] / total_values * 100
        
        print(f"  Outliers d√©tect√©s: {outliers_info['total_outliers']} ({outliers_info['percentage_outliers']:.2f}%)")
        
        return outliers_info
    
    def remove_outliers(self, df: pd.DataFrame, threshold: float = 3.0) -> pd.DataFrame:
        """
        Supprime les outliers des donn√©es.
        
        Args:
            df: DataFrame avec outliers
            threshold: Seuil en √©carts-types
            
        Returns:
            DataFrame sans outliers
        """
        print(f"üóëÔ∏è  Suppression des outliers (seuil: {threshold}œÉ)...")
        
        df_clean = df.copy()
        numeric_cols = df_clean.select_dtypes(include=[np.number]).columns.tolist()
        
        # Identifier les outliers
        mask = pd.Series([True] * len(df_clean))
        
        for col in numeric_cols:
            if col in ['unit_id', 'time_cycle', 'RUL']:
                continue
            
            values = df_clean[col]
            mean = values.mean()
            std = values.std()
            
            if std == 0:
                continue
            
            z_scores = np.abs((values - mean) / std)
            mask = mask & (z_scores <= threshold)
        
        outliers_removed = len(df_clean) - mask.sum()
        
        if outliers_removed > 0:
            print(f"  {outliers_removed} outliers supprim√©s ({outliers_removed/len(df_clean)*100:.1f}%)")
            df_clean = df_clean[mask].reset_index(drop=True)
        
        return df_clean
    
    def prepare_for_training(self, df: pd.DataFrame, target_col: str = 'RUL') -> Tuple[pd.DataFrame, pd.Series]:
        """
        Pr√©pare les donn√©es pour l'entra√Ænement.
        
        Args:
            df: DataFrame complet
            target_col: Nom de la colonne cible
            
        Returns:
            Tuple (X, y) pr√™ts pour l'entra√Ænement
        """
        print("üéØ Pr√©paration des donn√©es pour l'entra√Ænement...")
        
        # Copier les donn√©es
        df_prep = df.copy()
        
        # S√©parer les caract√©ristiques et la cible
        if target_col not in df_prep.columns:
            raise ValueError(f"Colonne cible '{target_col}' non trouv√©e")
        
        y = df_prep[target_col]
        X = df_prep.drop(columns=[target_col])
        
        # Exclure les colonnes d'identification
        exclude_cols = ['unit_id', 'time_cycle']
        X = X.drop(columns=[col for col in exclude_cols if col in X.columns])
        
        # V√©rifier les valeurs manquantes
        missing_cols = X.columns[X.isnull().any()].tolist()
        if missing_cols:
            print(f"  Imputation des valeurs manquantes dans {len(missing_cols)} colonnes...")
            for col in missing_cols:
                imputer = SimpleImputer(strategy='median')
                X[col] = imputer.fit_transform(X[[col]]).ravel()
        
        print(f"  X shape: {X.shape}, y shape: {y.shape}")
        
        return X, y
    
    def run_pipeline(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Ex√©cute le pipeline complet de pr√©traitement.
        
        Args:
            df: DataFrame brut
            
        Returns:
            DataFrame pr√©trait√©
        """
        print("\n" + "="*50)
        print("PIPELINE DE PR√âTRAITEMENT COMPLET")
        print("="*50)
        
        # 1. Nettoyage
        df_clean = self.clean_data(df)
        
        # 2. D√©tection des outliers
        outliers_info = self.detect_outliers(df_clean)
        
        # 3. Suppression des outliers
        df_no_outliers = self.remove_outliers(df_clean)
        
        # 4. Cr√©ation de caract√©ristiques
        df_features = self.create_features(df_no_outliers)
        
        # 5. Normalisation
        df_normalized = self.normalize_features(df_features, method='standard')
        
        print("\n" + "="*50)
        print("‚úÖ PR√âTRAITEMENT TERMIN√â")
        print("="*50)
        print(f"Shape initiale: {df.shape}")
        print(f"Shape finale: {df_normalized.shape}")
        print(f"Outliers trait√©s: {outliers_info['total_outliers']}")
        
        return df_normalized

def main():
    """Fonction principale pour tester le module."""
    print("="*50)
    print("TEST DU MODULE DE PR√âTRAITEMENT")
    print("="*50)
    
    # Cr√©er des donn√©es de test
    np.random.seed(42)
    n_samples = 1000
    
    test_data = pd.DataFrame({
        'unit_id': np.repeat(range(10), n_samples//10),
        'time_cycle': np.tile(range(n_samples//10), 10),
        'sensor_1': np.random.normal(100, 10, n_samples),
        'sensor_2': np.random.normal(50, 5, n_samples),
        'sensor_3': np.random.normal(20, 3, n_samples),
        'RUL': np.random.uniform(10, 200, n_samples)
    })
    
    # Ajouter quelques outliers
    test_data.loc[::100, 'sensor_1'] = 500
    test_data.loc[::50, 'sensor_2'] = 200
    
    # Initialiser le pr√©processeur
    preprocessor = DataPreprocessor()
    
    # Ex√©cuter le pipeline
    processed_data = preprocessor.run_pipeline(test_data)
    
    print(f"\nüìã DONN√âES TRAIT√âES:")
    print(f"  Colonnes: {list(processed_data.columns)}")
    print(f"  Shape: {processed_data.shape}")
    
    return processed_data

if __name__ == "__main__":
    processed_data = main()
