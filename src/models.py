"""
Module de mod√®les de Machine Learning pour la maintenance pr√©dictive.
LSTM, XGBoost, Random Forest pour la pr√©diction de RUL.
"""

import numpy as np
import pandas as pd
from typing import Dict, Tuple, Any, List
import pickle
import joblib
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns

# Machine Learning
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
import xgboost as xgb

# Deep Learning
import tensorflow as tf
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from tensorflow.keras.optimizers import Adam

class PredictiveModels:
    """
    Classe pour entra√Æner et g√©rer les mod√®les de pr√©diction.
    
    Attributes:
        models (dict): Dictionnaire des mod√®les entra√Æn√©s
        histories (dict): Historique d'entra√Ænement des mod√®les
        scaler (StandardScaler): Scaler pour les caract√©ristiques
    """
    
    def __init__(self, config: Dict = None):
        """
        Initialise les mod√®les de pr√©diction.
        
        Args:
            config: Configuration des mod√®les
        """
        self.config = config or {}
        self.models = {}
        self.histories = {}
        self.scaler = StandardScaler()
        
        # Configuration TensorFlow
        tf.random.set_seed(42)
        
        print("‚úÖ Mod√®les de pr√©diction initialis√©s")
    
    def create_lstm_model(self, input_shape: Tuple) -> Model:
        """
        Cr√©e un mod√®le LSTM pour la pr√©diction de RUL.
        
        Args:
            input_shape: Forme des donn√©es d'entr√©e (timesteps, features)
            
        Returns:
            Mod√®le LSTM compil√©
        """
        print("ü§ñ Cr√©ation du mod√®le LSTM...")
        
        model = Sequential([
            Input(shape=input_shape),
            LSTM(128, return_sequences=True),
            BatchNormalization(),
            Dropout(0.2),
            
            LSTM(64, return_sequences=False),
            BatchNormalization(),
            Dropout(0.2),
            
            Dense(32, activation='relu'),
            BatchNormalization(),
            Dropout(0.1),
            
            Dense(16, activation='relu'),
            Dense(1)  # Pr√©diction RUL
        ])
        
        # Compiler le mod√®le
        optimizer = Adam(learning_rate=0.001)
        model.compile(
            optimizer=optimizer,
            loss='mse',
            metrics=['mae', 'mse']
        )
        
        print(f"  ‚úÖ Mod√®le LSTM cr√©√©: {model.summary()}")
        
        return model
    
    def create_xgboost_model(self) -> xgb.XGBRegressor:
        """
        Cr√©e un mod√®le XGBoost.
        
        Returns:
            Mod√®le XGBoost configur√©
        """
        print("ü§ñ Cr√©ation du mod√®le XGBoost...")
        
        model = xgb.XGBRegressor(
            n_estimators=100,
            max_depth=6,
            learning_rate=0.1,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42,
            n_jobs=-1,
            verbosity=0
        )
        
        print("  ‚úÖ Mod√®le XGBoost cr√©√©")
        
        return model
    
    def create_random_forest_model(self) -> RandomForestRegressor:
        """
        Cr√©e un mod√®le Random Forest.
        
        Returns:
            Mod√®le Random Forest configur√©
        """
        print("ü§ñ Cr√©ation du mod√®le Random Forest...")
        
        model = RandomForestRegressor(
            n_estimators=100,
            max_depth=10,
            min_samples_split=2,
            min_samples_leaf=1,
            random_state=42,
            n_jobs=-1
        )
        
        print("  ‚úÖ Mod√®le Random Forest cr√©√©")
        
        return model
    
    def train_lstm(self, X_train: np.ndarray, y_train: np.ndarray, 
                   X_val: np.ndarray = None, y_val: np.ndarray = None,
                   epochs: int = 50, batch_size: int = 32) -> Tuple[Model, Dict]:
        """
        Entra√Æne un mod√®le LSTM.
        
        Args:
            X_train, y_train: Donn√©es d'entra√Ænement
            X_val, y_val: Donn√©es de validation
            epochs: Nombre d'√©poques
            batch_size: Taille des lots
            
        Returns:
            Tuple (mod√®le, historique)
        """
        print("\n" + "="*50)
        print("ENTRA√éNEMENT DU MOD√àLE LSTM")
        print("="*50)
        
        # Cr√©er le mod√®le
        model = self.create_lstm_model(X_train.shape[1:])
        
        # Callbacks
        callbacks = [
            EarlyStopping(
                monitor='val_loss',
                patience=10,
                restore_best_weights=True,
                verbose=1
            ),
            ModelCheckpoint(
                'models/lstm_best.h5',
                monitor='val_loss',
                save_best_only=True,
                verbose=1
            ),
            ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=5,
                min_lr=1e-6,
                verbose=1
            )
        ]
        
        # Entra√Æner le mod√®le
        if X_val is not None:
            validation_data = (X_val, y_val)
            validation_split = None
        else:
            validation_data = None
            validation_split = 0.2
        
        history = model.fit(
            X_train, y_train,
            epochs=epochs,
            batch_size=batch_size,
            validation_split=validation_split,
            validation_data=validation_data,
            callbacks=callbacks,
            verbose=1
        )
        
        # Sauvegarder
        self.models['lstm'] = model
        self.histories['lstm'] = history.history
        
        print("‚úÖ LSTM entra√Æn√©")
        
        return model, history.history
    
    def train_xgboost(self, X_train: np.ndarray, y_train: np.ndarray) -> xgb.XGBRegressor:
        """
        Entra√Æne un mod√®le XGBoost.
        
        Args:
            X_train, y_train: Donn√©es d'entra√Ænement
            
        Returns:
            Mod√®le XGBoost entra√Æn√©
        """
        print("\n" + "="*50)
        print("ENTRA√éNEMENT DU MOD√àLE XGBOOST")
        print("="*50)
        
        # Aplatir les donn√©es si n√©cessaire (pour s√©quences LSTM)
        if len(X_train.shape) == 3:
            X_train_flat = X_train.reshape(X_train.shape[0], -1)
        else:
            X_train_flat = X_train
        
        # Cr√©er et entra√Æner le mod√®le
        model = self.create_xgboost_model()
        
        print("  Entra√Ænement en cours...")
        model.fit(X_train_flat, y_train)
        
        # Validation crois√©e
        cv_scores = cross_val_score(model, X_train_flat, y_train, 
                                   cv=5, scoring='neg_mean_absolute_error')
        
        print(f"  Scores de validation crois√©e (MAE): {-cv_scores.mean():.2f} ¬± {cv_scores.std():.2f}")
        
        # Sauvegarder
        self.models['xgboost'] = model
        
        print("‚úÖ XGBoost entra√Æn√©")
        
        return model
    
    def train_random_forest(self, X_train: np.ndarray, y_train: np.ndarray) -> RandomForestRegressor:
        """
        Entra√Æne un mod√®le Random Forest.
        
        Args:
            X_train, y_train: Donn√©es d'entra√Ænement
            
        Returns:
            Mod√®le Random Forest entra√Æn√©
        """
        print("\n" + "="*50)
        print("ENTRA√éNEMENT DU MOD√àLE RANDOM FOREST")
        print("="*50)
        
        # Aplatir les donn√©es si n√©cessaire
        if len(X_train.shape) == 3:
            X_train_flat = X_train.reshape(X_train.shape[0], -1)
        else:
            X_train_flat = X_train
        
        # Cr√©er et entra√Æner le mod√®le
        model = self.create_random_forest_model()
        
        print("  Entra√Ænement en cours...")
        model.fit(X_train_flat, y_train)
        
        # Importance des caract√©ristiques
        feature_importance = model.feature_importances_
        top_features = np.argsort(feature_importance)[-5:][::-1]
        
        print("  Top 5 caract√©ristiques importantes:")
        for i, idx in enumerate(top_features, 1):
            print(f"    {i}. Caract√©ristique {idx}: {feature_importance[idx]:.4f}")
        
        # Sauvegarder
        self.models['random_forest'] = model
        
        print("‚úÖ Random Forest entra√Æn√©")
        
        return model
    
    def train_ensemble(self, X_train: np.ndarray, y_train: np.ndarray) -> Dict:
        """
        Entra√Æne un mod√®le ensemble combinant les pr√©dictions.
        
        Args:
            X_train, y_train: Donn√©es d'entra√Ænement
            
        Returns:
            Mod√®le ensemble
        """
        print("\n" + "="*50)
        print("ENTRA√éNEMENT DU MOD√àLE ENSEMBLE")
        print("="*50)
        
        # V√©rifier que tous les mod√®les sont entra√Æn√©s
        required_models = ['lstm', 'xgboost', 'random_forest']
        for model_name in required_models:
            if model_name not in self.models:
                print(f"  ‚ö†Ô∏è  Mod√®le {model_name} non entra√Æn√©. Entra√Ænement en cours...")
                if model_name == 'lstm':
                    self.train_lstm(X_train, y_train)
                elif model_name == 'xgboost':
                    self.train_xgboost(X_train, y_train)
                elif model_name == 'random_forest':
                    self.train_random_forest(X_train, y_train)
        
        # Cr√©er le mod√®le ensemble
        ensemble_model = {
            'type': 'ensemble',
            'models': self.models,
            'weights': {'lstm': 0.4, 'xgboost': 0.4, 'random_forest': 0.2}
        }
        
        self.models['ensemble'] = ensemble_model
        
        print("‚úÖ Mod√®le ensemble cr√©√©")
        
        return ensemble_model
    
    def evaluate_model(self, model_name: str, X_test: np.ndarray, y_test: np.ndarray) -> Dict:
        """
        √âvalue un mod√®le sp√©cifique.
        
        Args:
            model_name: Nom du mod√®le
            X_test, y_test: Donn√©es de test
            
        Returns:
            M√©triques d'√©valuation
        """
        if model_name not in self.models:
            raise ValueError(f"Mod√®le {model_name} non disponible")
        
        print(f"\nüìä √âvaluation du mod√®le {model_name}...")
        
        model = self.models[model_name]
        
        # Pr√©parer les donn√©es selon le type de mod√®le
        if model_name == 'lstm':
            y_pred = model.predict(X_test).flatten()
        elif model_name == 'ensemble':
            # Moyenne pond√©r√©e des pr√©dictions
            predictions = []
            for submodel_name, submodel in model['models'].items():
                if submodel_name == 'lstm':
                    pred = submodel.predict(X_test).flatten()
                else:
                    # Aplatir pour les mod√®les non-s√©quentiels
                    if len(X_test.shape) == 3:
                        X_test_flat = X_test.reshape(X_test.shape[0], -1)
                    else:
                        X_test_flat = X_test
                    pred = submodel.predict(X_test_flat)
                predictions.append(pred)
            
            # Moyenne pond√©r√©e
            weights = model['weights']
            y_pred = sum(p * weights[name] for name, p in zip(model['models'].keys(), predictions))
        else:
            # Aplatir pour les mod√®les non-s√©quentiels
            if len(X_test.shape) == 3:
                X_test_flat = X_test.reshape(X_test.shape[0], -1)
            else:
                X_test_flat = X_test
            y_pred = model.predict(X_test_flat)
        
        # Calculer les m√©triques
        metrics = self._calculate_metrics(y_test, y_pred)
        
        print(f"  MAE: {metrics['mae']:.2f}")
        print(f"  RMSE: {metrics['rmse']:.2f}")
        print(f"  R¬≤: {metrics['r2']:.4f}")
        
        return metrics
    
    def _calculate_metrics(self, y_true: np.ndarray, y_pred: np.ndarray) -> Dict:
        """
        Calcule les m√©triques d'√©valuation.
        
        Args:
            y_true: Valeurs r√©elles
            y_pred: Valeurs pr√©dites
            
        Returns:
            Dict avec les m√©triques
        """
        mae = mean_absolute_error(y_true, y_pred)
        rmse = np.sqrt(mean_squared_error(y_true, y_pred))
        r2 = r2_score(y_true, y_pred)
        
        # Erreur relative moyenne
        mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-10))) * 100
        
        return {
            'mae': mae,
            'rmse': rmse,
            'r2': r2,
            'mape': mape,
            'predictions': y_pred.tolist()
        }
    
    def train_all_models(self, X_train: np.ndarray, y_train: np.ndarray) -> Dict:
        """
        Entra√Æne tous les mod√®les.
        
        Args:
            X_train, y_train: Donn√©es d'entra√Ænement
            
        Returns:
            Dict avec tous les mod√®les entra√Æn√©s
        """
        print("\n" + "="*50)
        print("üöÄ ENTRA√éNEMENT DE TOUS LES MOD√àLES")
        print("="*50)
        
        # Split validation
        X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(
            X_train, y_train, test_size=0.2, random_state=42
        )
        
        # Entra√Æner LSTM
        lstm_model, lstm_history = self.train_lstm(
            X_train_split, y_train_split, 
            X_val_split, y_val_split
        )
        
        # Entra√Æner XGBoost
        xgb_model = self.train_xgboost(X_train_split, y_train_split)
        
        # Entra√Æner Random Forest
        rf_model = self.train_random_forest(X_train_split, y_train_split)
        
        # Cr√©er ensemble
        ensemble_model = self.train_ensemble(X_train_split, y_train_split)
        
        print("\n" + "="*50)
        print("‚úÖ TOUS LES MOD√àLES ENTRA√éN√âS")
        print("="*50)
        
        return {
            'lstm': lstm_model,
            'xgboost': xgb_model,
            'random_forest': rf_model,
            'ensemble': ensemble_model,
            'lstm_history': lstm_history
        }
    
    def save_models(self, path: str = "models"):
        """
        Sauvegarde tous les mod√®les.
        
        Args:
            path: Chemin de sauvegarde
        """
        import os
        os.makedirs(path, exist_ok=True)
        
        print(f"\nüíæ Sauvegarde des mod√®les dans {path}/")
        
        for name, model in self.models.items():
            if name == 'lstm':
                model.save(f"{path}/{name}_model.h5")
            elif name == 'ensemble':
                # Sauvegarder les mod√®les individuels de l'ensemble
                for submodel_name, submodel in model['models'].items():
                    if submodel_name == 'lstm':
                        submodel.save(f"{path}/ensemble_{submodel_name}_model.h5")
                    else:
                        joblib.dump(submodel, f"{path}/ensemble_{submodel_name}_model.pkl")
                # Sauvegarder la configuration de l'ensemble
                with open(f"{path}/ensemble_config.pkl", 'wb') as f:
                    pickle.dump({'weights': model['weights']}, f)
            else:
                joblib.dump(model, f"{path}/{name}_model.pkl")
            
            print(f"  ‚úÖ {name} sauvegard√©")
    
    def load_models(self, path: str = "models"):
        """
        Charge les mod√®les sauvegard√©s.
        
        Args:
            path: Chemin des mod√®les
        """
        import os
        
        print(f"\nüìÇ Chargement des mod√®les depuis {path}/")
        
        # Charger LSTM
        lstm_path = f"{path}/lstm_model.h5"
        if os.path.exists(lstm_path):
            self.models['lstm'] = tf.keras.models.load_model(lstm_path)
            print("  ‚úÖ LSTM charg√©")
        
        # Charger XGBoost
        xgb_path = f"{path}/xgboost_model.pkl"
        if os.path.exists(xgb_path):
            self.models['xgboost'] = joblib.load(xgb_path)
            print("  ‚úÖ XGBoost charg√©")
        
        # Charger Random Forest
        rf_path = f"{path}/random_forest_model.pkl"
        if os.path.exists(rf_path):
            self.models['random_forest'] = joblib.load(rf_path)
            print("  ‚úÖ Random Forest charg√©")
        
        # Charger Ensemble
        ensemble_config_path = f"{path}/ensemble_config.pkl"
        if os.path.exists(ensemble_config_path):
            with open(ensemble_config_path, 'rb') as f:
                ensemble_config = pickle.load(f)
            
            # Charger les mod√®les de l'ensemble
            ensemble_models = {}
            for model_name in ['lstm', 'xgboost', 'random_forest']:
                if model_name == 'lstm':
                    model_path = f"{path}/ensemble_{model_name}_model.h5"
                    if os.path.exists(model_path):
                        ensemble_models[model_name] = tf.keras.models.load_model(model_path)
                else:
                    model_path = f"{path}/ensemble_{model_name}_model.pkl"
                    if os.path.exists(model_path):
                        ensemble_models[model_name] = joblib.load(model_path)
            
            if ensemble_models:
                self.models['ensemble'] = {
                    'type': 'ensemble',
                    'models': ensemble_models,
                    'weights': ensemble_config['weights']
                }
                print("  ‚úÖ Ensemble charg√©")
    
    def predict(self, X: np.ndarray, model_name: str = 'ensemble') -> np.ndarray:
        """
        Fait une pr√©diction avec le mod√®le sp√©cifi√©.
        
        Args:
            X: Donn√©es d'entr√©e
            model_name: Nom du mod√®le
            
        Returns:
            Pr√©dictions
        """
        if model_name not in self.models:
            raise ValueError(f"Mod√®le {model_name} non disponible")
        
        model = self.models[model_name]
        
        if model_name == 'lstm':
            return model.predict(X).flatten()
        elif model_name == 'ensemble':
            # Combinaison des pr√©dictions
            predictions = []
            weights = []
            
            for submodel_name, submodel in model['models'].items():
                if submodel_name == 'lstm':
                    pred = submodel.predict(X).flatten()
                else:
                    # Aplatir pour les mod√®les non-s√©quentiels
                    if len(X.shape) == 3:
                        X_flat = X.reshape(X.shape[0], -1)
                    else:
                        X_flat = X
                    pred = submodel.predict(X_flat)
                
                predictions.append(pred)
                weights.append(model['weights'][submodel_name])
            
            # Moyenne pond√©r√©e
            weighted_sum = sum(p * w for p, w in zip(predictions, weights))
            return weighted_sum
        else:
            # Aplatir pour les mod√®les non-s√©quentiels
            if len(X.shape) == 3:
                X_flat = X.reshape(X.shape[0], -1)
            else:
                X_flat = X
            return model.predict(X_flat)
    
    def plot_training_history(self, model_name: str = 'lstm'):
        """
        Affiche l'historique d'entra√Ænement d'un mod√®le.
        
        Args:
            model_name: Nom du mod√®le
        """
        if model_name not in self.histories:
            print(f"Historique non disponible pour {model_name}")
            return
        
        history = self.histories[model_name]
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
        
        # Loss
        ax1.plot(history['loss'], label='Entra√Ænement')
        ax1.plot(history['val_loss'], label='Validation')
        ax1.set_title('Loss du mod√®le')
        ax1.set_xlabel('√âpoque')
        ax1.set_ylabel('Loss')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # MAE
        ax2.plot(history['mae'], label='Entra√Ænement')
        ax2.plot(history['val_mae'], label='Validation')
        ax2.set_title('MAE du mod√®le')
        ax2.set_xlabel('√âpoque')
        ax2.set_ylabel('MAE')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()

def main():
    """Fonction principale pour tester le module."""
    print("="*50)
    print("TEST DU MODULE DE MOD√àLES")
    print("="*50)
    
    # G√©n√©rer des donn√©es de test
    np.random.seed(42)
    n_samples = 1000
    sequence_length = 50
    n_features = 21
    
    # Donn√©es LSTM (3D)
    X_lstm = np.random.randn(n_samples, sequence_length, n_features)
    y_lstm = np.random.uniform(10, 200, n_samples)
    
    # Donn√©es pour mod√®les classiques (2D)
    X_flat = X_lstm.reshape(n_samples, -1)
    
    # Split train/test
    X_train, X_test, y_train, y_test = train_test_split(
        X_lstm, y_lstm, test_size=0.2, random_state=42
    )
    
    # Initialiser les mod√®les
    pm = PredictiveModels()
    
    # Entra√Æner tous les mod√®les
    models = pm.train_all_models(X_train, y_train)
    
    # √âvaluer les mod√®les
    print("\n" + "="*50)
    print("√âVALUATION DES MOD√àLES")
    print("="*50)
    
    results = {}
    for model_name in ['lstm', 'xgboost', 'random_forest', 'ensemble']:
        metrics = pm.evaluate_model(model_name, X_test, y_test)
        results[model_name] = metrics
    
    # Afficher les r√©sultats
    print("\nüìã R√âSULTATS FINAUX:")
    print("-" * 40)
    print(f"{'Mod√®le':<20} {'MAE':<10} {'RMSE':<10} {'R¬≤':<10}")
    print("-" * 40)
    
    for model_name, metrics in results.items():
        print(f"{model_name:<20} {metrics['mae']:<10.2f} {metrics['rmse']:<10.2f} {metrics['r2']:<10.4f}")
    
    # Sauvegarder les mod√®les
    pm.save_models('test_models')
    
    return results

if __name__ == "__main__":
    results = main()
